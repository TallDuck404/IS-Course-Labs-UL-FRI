{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1: Entity Recognition and Replacement\n",
    "- **Objective:** Recognize named entities in imdb dataset (such as person names, locations) and replace them with generic labels to anonymize text data, if necessary.\n",
    "- **Instructions:**\n",
    "  - Utilize Stanza's entity recognition capabilities to identify named entities within the text.\n",
    "  - Replace recognized named entities with generic labels or placeholders to ensure anonymity in the text.\n",
    "\n",
    "### Task 2: Removal of HTML Tags or URLs\n",
    "- **Objective:** Enhance the preprocess function to eliminate HTML tags and URLs from the text data in imdb dataset.\n",
    "- **Instructions:**\n",
    "  - Update the preprocess function using regular expressions to remove HTML tags (e.g., <tag>) and URLs present within the text.\n",
    "  - Ensure the removal of HTML tags and URLs to clean the text data effectively.\n",
    "\n",
    "### Task 3: Comparison of Pre-trained Word Embeddings\n",
    "- **Objective:** Download various pre-trained word embeddings (e.g., FastText, Word2Vec, GloVe) and evaluate their performance against custom-trained embeddings on imdb dataset.\n",
    "- **Instructions:**\n",
    "  - Access the NLPL Vector Repository http://vectors.nlpl.eu/repository/ to download pre-trained word embeddings such as FastText, Word2Vec, and GloVe.\n",
    "  - Compare these pre-trained embeddings with our custom-trained embeddings on imdb dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7dbdfbaddddbfb0"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Download the IMDb dataset\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "# Select 1,000 examples from each split (train and test)\n",
    "data = pd.DataFrame(imdb_dataset['train'].shuffle(seed=42).select(range(20000)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:02:56.572611Z",
     "start_time": "2024-01-03T14:02:53.994332Z"
    }
   },
   "id": "df28819214dc1c11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TASK 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f66488f545d5ac75"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 24.0MB/s]                    \n",
      "2024-01-03 15:02:36 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-01-03 15:02:38 INFO: File exists: /Users/azagar/stanza_resources/en/default.zip\n",
      "2024-01-03 15:02:43 INFO: Finished downloading models and saved to /Users/azagar/stanza_resources.\n",
      "2024-01-03 15:02:43 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json: 367kB [00:00, 26.6MB/s]                    \n",
      "2024-01-03 15:02:44 INFO: Loading these models for language: en (English):\n",
      "================================\n",
      "| Processor | Package          |\n",
      "--------------------------------\n",
      "| tokenize  | combined         |\n",
      "| ner       | ontonotes_charlm |\n",
      "================================\n",
      "\n",
      "2024-01-03 15:02:44 INFO: Using device: cpu\n",
      "2024-01-03 15:02:44 INFO: Loading: tokenize\n",
      "2024-01-03 15:02:44 INFO: Loading: ner\n",
      "2024-01-03 15:02:45 INFO: Done loading processors!\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Initialize Stanza pipeline with NER\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "\n",
    "def anonymize_entities(text):\n",
    "    doc = nlp(text)\n",
    "    for entity in doc.ents:\n",
    "        text = text.replace(entity.text, '[ANONYMIZED]')\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "anonymized_text = [anonymize_entities(text) for text in data['text'] ]\n",
    "print(anonymized_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:02:50.088381Z",
     "start_time": "2024-01-03T14:02:36.482695Z"
    }
   },
   "id": "471c6a8b84cd1bdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edff8ba5b541f3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Add other preprocessing steps here if needed\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Check out this link: <a href='http://example.com'>Example</a>\"\n",
    "clean_text = preprocess_text(sample_text)\n",
    "print(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:02:50.090311Z",
     "start_time": "2024-01-03T14:02:50.089924Z"
    }
   },
   "id": "3194e3e3c2414ce3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f19b2ecb9310363c"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/azagar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/azagar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/azagar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "'relation fortier profiler fact police series violent crime profiler look crispy fortier look classic profiler plot quite simple fortier plot far complicated fortier look like prime suspect spot similarity main character weak weirdo clairvoyance people like compare judge evaluate enjoying funny thing people writing fortier look american hand arguing prefer american series maybe language spirit think series english american way actor really good funny acting superficial'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Lemmatizer and Stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())  # Convert text to lowercase\n",
    "\n",
    "    # Remove punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Stemming (uncomment if you want to use stemming)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the words back into a string\n",
    "    preprocessed_text = ' '.join(lemmatized_words)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "data['clean_text'] = data['text'].apply(preprocess_text)\n",
    "# Check preprocessed first instance\n",
    "data['clean_text'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:04:05.019986Z",
     "start_time": "2024-01-03T14:03:03.486477Z"
    }
   },
   "id": "10ca1d4c981016f0"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['clean_text', 'text']], data['label'], test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:04:05.120107Z",
     "start_time": "2024-01-03T14:04:05.061236Z"
    }
   },
   "id": "16ea191900d7e41e"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download embeddings -> https://github.com/piskvorky/gensim-data\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "# word2vec_model = api.load(\"glove-twitter-25\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:04:40.923962Z",
     "start_time": "2024-01-03T14:04:05.122172Z"
    }
   },
   "id": "1677b938a71b1c6e"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.key_to_index]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(word2vec_model[doc], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:04:40.974715Z",
     "start_time": "2024-01-03T14:04:40.880685Z"
    }
   },
   "id": "c8bd800b81d04d84"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Add Word2Vec representations to DataFrame\n",
    "X_train_w2v = [document_vector(word2vec_model, text.lower().split()) for text in X_train['clean_text']]\n",
    "X_test_w2v = [document_vector(word2vec_model, text.lower().split()) for text in X_test['clean_text']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:04:49.710415Z",
     "start_time": "2024-01-03T14:04:40.956410Z"
    }
   },
   "id": "a58367aaa20f29c7"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.856\n",
      "Random Forest Accuracy: 0.81375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_w2v, y_train)\n",
    "logistic_predictions = logistic_model.predict(X_test_w2v)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_accuracy)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_w2v, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_w2v)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T14:05:12.178627Z",
     "start_time": "2024-01-03T14:04:49.741437Z"
    }
   },
   "id": "74f5c094809af157"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
